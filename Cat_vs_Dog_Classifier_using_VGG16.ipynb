{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Cat_vs_Dog_Classifier_using_VGG16.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "[View in Colaboratory](https://colab.research.google.com/github/gauravbansal98/Cat-vs-Dog-Classifier-Using-CNN/blob/master/Cat_vs_Dog_Classifier_using_VGG16.ipynb)"
      ]
    },
    {
      "metadata": {
        "id": "MlkJawZMQCzm",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 133
        },
        "outputId": "f84603c6-89fa-4abf-8a19-96edef39ecba"
      },
      "cell_type": "code",
      "source": [
        "!apt-get install -y -qq software-properties-common python-software-properties module-init-tools\n",
        "!add-apt-repository -y ppa:alessandro-strada/ppa 2>&1 > /dev/null\n",
        "!apt-get update -qq 2>&1 > /dev/null\n",
        "!apt-get -y install -qq google-drive-ocamlfuse fuse\n",
        "from google.colab import auth\n",
        "auth.authenticate_user()\n",
        "from oauth2client.client import GoogleCredentials\n",
        "creds = GoogleCredentials.get_application_default()\n",
        "import getpass\n",
        "!google-drive-ocamlfuse -headless -id={creds.client_id} -secret={creds.client_secret} < /dev/null 2>&1 | grep URL\n",
        "vcode = getpass.getpass()\n",
        "!echo {vcode} | google-drive-ocamlfuse -headless -id={creds.client_id} -secret={creds.client_secret}"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "gpg: keybox '/tmp/tmp92m1l23v/pubring.gpg' created\n",
            "gpg: /tmp/tmp92m1l23v/trustdb.gpg: trustdb created\n",
            "gpg: key AD5F235DF639B041: public key \"Launchpad PPA for Alessandro Strada\" imported\n",
            "gpg: Total number processed: 1\n",
            "gpg:               imported: 1\n",
            "Warning: apt-key output should not be parsed (stdout is not a terminal)\n",
            "··········\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "5sApX-rXRAc8",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!mkdir -p drive\n",
        "!google-drive-ocamlfuse drive"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "XjafQ99tRQia",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.chdir(\"drive/Cat vs Dog Classifier\")\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import cv2\n",
        "from random import shuffle"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "rdD3KGwI1IAK",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "TRAIN_DIR = 'training_data'\n",
        "IMG_SIZE = 224\n",
        "data_to_save_dir = 'data'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "HI55RXAB1Pph",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def label_img(img):\n",
        "    word_label = img.split('.')[-3]\n",
        "    if word_label == 'cat': return [1,0]\n",
        "    elif word_label == 'dog': return [0,1]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "EYvDdnoV8OpQ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import inspect\n",
        "import os\n",
        "\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import time\n",
        "\n",
        "VGG_MEAN = [103.939, 116.779, 123.68]\n",
        "\n",
        "\n",
        "class Vgg16:\n",
        "    def __init__(self, vgg16_npy_path=None):\n",
        "        if vgg16_npy_path is None:\n",
        "            path = inspect.getfile(Vgg16)\n",
        "            path = os.path.abspath(os.path.join(path, os.pardir))\n",
        "            path = os.path.join(path, \"vgg16.npy\")\n",
        "            vgg16_npy_path = path\n",
        "            print(path)\n",
        "\n",
        "        self.data_dict = np.load(vgg16_npy_path, encoding='latin1').item()\n",
        "        print(\"npy file loaded\")\n",
        "\n",
        "    def build(self, rgb):\n",
        "        \"\"\"\n",
        "        load variable from npy to build the VGG\n",
        "        :param rgb: rgb image [batch, height, width, 3] values scaled [0, 1]\n",
        "        \"\"\"\n",
        "\n",
        "        start_time = time.time()\n",
        "        print(\"build model started\")\n",
        "        #rgb_scaled = rgb * 255.0\n",
        "\n",
        "        # Convert RGB to BGR\n",
        "        red, green, blue = tf.split(axis=3, num_or_size_splits=3, value=rgb)\n",
        "        assert red.get_shape().as_list()[1:] == [224, 224, 1]\n",
        "        assert green.get_shape().as_list()[1:] == [224, 224, 1]\n",
        "        assert blue.get_shape().as_list()[1:] == [224, 224, 1]\n",
        "        bgr = tf.concat(axis=3, values=[\n",
        "            blue - VGG_MEAN[0],\n",
        "            green - VGG_MEAN[1],\n",
        "            red - VGG_MEAN[2],\n",
        "        ])\n",
        "        assert bgr.get_shape().as_list()[1:] == [224, 224, 3]\n",
        "\n",
        "        self.conv1_1 = self.conv_layer(bgr, \"conv1_1\")\n",
        "        self.conv1_2 = self.conv_layer(self.conv1_1, \"conv1_2\")\n",
        "        self.pool1 = self.max_pool(self.conv1_2, 'pool1')\n",
        "\n",
        "        self.conv2_1 = self.conv_layer(self.pool1, \"conv2_1\")\n",
        "        self.conv2_2 = self.conv_layer(self.conv2_1, \"conv2_2\")\n",
        "        self.pool2 = self.max_pool(self.conv2_2, 'pool2')\n",
        "\n",
        "        self.conv3_1 = self.conv_layer(self.pool2, \"conv3_1\")\n",
        "        self.conv3_2 = self.conv_layer(self.conv3_1, \"conv3_2\")\n",
        "        self.conv3_3 = self.conv_layer(self.conv3_2, \"conv3_3\")\n",
        "        self.pool3 = self.max_pool(self.conv3_3, 'pool3')\n",
        "\n",
        "        self.conv4_1 = self.conv_layer(self.pool3, \"conv4_1\")\n",
        "        self.conv4_2 = self.conv_layer(self.conv4_1, \"conv4_2\")\n",
        "        self.conv4_3 = self.conv_layer(self.conv4_2, \"conv4_3\")\n",
        "        self.pool4 = self.max_pool(self.conv4_3, 'pool4')\n",
        "\n",
        "        self.conv5_1 = self.conv_layer(self.pool4, \"conv5_1\")\n",
        "        self.conv5_2 = self.conv_layer(self.conv5_1, \"conv5_2\")\n",
        "        self.conv5_3 = self.conv_layer(self.conv5_2, \"conv5_3\")\n",
        "        self.pool5 = self.max_pool(self.conv5_3, 'pool5')\n",
        "\n",
        "        self.fc6 = self.fc_layer(self.pool5, \"fc6\")\n",
        "        assert self.fc6.get_shape().as_list()[1:] == [4096]\n",
        "        self.relu6 = tf.nn.relu(self.fc6)\n",
        "\n",
        "        self.fc7 = self.fc_layer(self.relu6, \"fc7\")\n",
        "        self.relu7 = tf.nn.relu(self.fc7)\n",
        "\n",
        "        self.data_dict = None\n",
        "        print((\"build model finished: %ds\" % (time.time() - start_time)))\n",
        "        \n",
        "        return self.relu7\n",
        "\n",
        "    def avg_pool(self, bottom, name):\n",
        "        return tf.nn.avg_pool(bottom, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME', name=name)\n",
        "\n",
        "    def max_pool(self, bottom, name):\n",
        "        return tf.nn.max_pool(bottom, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME', name=name)\n",
        "\n",
        "    def conv_layer(self, bottom, name):\n",
        "        with tf.variable_scope(name):\n",
        "            filt = self.get_conv_filter(name)\n",
        "\n",
        "            conv = tf.nn.conv2d(bottom, filt, [1, 1, 1, 1], padding='SAME')\n",
        "\n",
        "            conv_biases = self.get_bias(name)\n",
        "            bias = tf.nn.bias_add(conv, conv_biases)\n",
        "\n",
        "            relu = tf.nn.relu(bias)\n",
        "            return relu\n",
        "\n",
        "    def fc_layer(self, bottom, name):\n",
        "        with tf.variable_scope(name):\n",
        "            shape = bottom.get_shape().as_list()\n",
        "            dim = 1\n",
        "            for d in shape[1:]:\n",
        "                dim *= d\n",
        "            x = tf.reshape(bottom, [-1, dim])\n",
        "\n",
        "            weights = self.get_fc_weight(name)\n",
        "            biases = self.get_bias(name)\n",
        "\n",
        "            # Fully connected layer. Note that the '+' operation automatically\n",
        "            # broadcasts the biases.\n",
        "            fc = tf.nn.bias_add(tf.matmul(x, weights), biases)\n",
        "\n",
        "            return fc\n",
        "\n",
        "    def get_conv_filter(self, name):\n",
        "        return tf.constant(self.data_dict[name][0], name=\"filter\")\n",
        "\n",
        "    def get_bias(self, name):\n",
        "        return tf.constant(self.data_dict[name][1], name=\"biases\")\n",
        "\n",
        "    def get_fc_weight(self, name):\n",
        "        return tf.constant(self.data_dict[name][0], name=\"weights\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "_mNs1DQVfh9h",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "X = tf.placeholder('float', [None, 224, 224, 3])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "flxAPevSjq29",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67
        },
        "outputId": "89eeed97-2b46-4310-fa8a-d2d418f5d682"
      },
      "cell_type": "code",
      "source": [
        "vgg = Vgg16('vgg16.npy')\n",
        "class_output = vgg.build(X)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "npy file loaded\n",
            "build model started\n",
            "build model finished: 2s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "ebg4rsSsids_",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "training_data = []\n",
        "i = 0\n",
        "with tf.Session() as sess:\n",
        "  sess.run(tf.global_variables_initializer())\n",
        "  for img in os.listdir(TRAIN_DIR):\n",
        "    print(i)\n",
        "    i = i + 1\n",
        "    label = label_img(img)\n",
        "    path = os.path.join(TRAIN_DIR,img)\n",
        "    img = cv2.imread(path)\n",
        "    img = cv2.resize(img, (IMG_SIZE,IMG_SIZE))\n",
        "    img = np.reshape(np.array(img), (1, 224, 224, 3))\n",
        "    img = sess.run(class_output, feed_dict = {X:img})\n",
        "    training_data.append([np.array(img),np.array(label)])\n",
        "  shuffle(training_data)\n",
        "  np.save('bottleneck_layer.npy', training_data)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Y1L4O_IykCgy",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "training_data = np.load('bottleneck_layer.npy')\n",
        "training_x = []\n",
        "training_y = []\n",
        "testing_x = []\n",
        "testing_y = []\n",
        "for i in range(23552):\n",
        "  training_x.append(training_data[i][0])\n",
        "  training_y.append(training_data[i][1])\n",
        "  \n",
        "for i in range(1148):\n",
        "  testing_x.append(training_data[23552+i][0])\n",
        "  testing_y.append(training_data[23552+i][1])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "EsRoOq5uqLQV",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "tf.reset_default_graph()\n",
        "hm_epochs = 10\n",
        "\n",
        "\n",
        "X = tf.placeholder(dtype = tf.float32, shape = [None, 4096,], name = 'input_labels')\n",
        "y = tf.placeholder(dtype = tf.int32, shape = [None, 2], name = 'output_labels')\n",
        "learning_rate = tf.placeholder('float')\n",
        "weights = {'weight' : tf.Variable(tf.random_normal(shape = [4096, 2])), 'bias' : tf.Variable(tf.random_normal((1, 2)))}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "rWzzD-EYxEk0",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        },
        "outputId": "c2fc85d3-c8f0-44c7-e7bf-b5d497dec459"
      },
      "cell_type": "code",
      "source": [
        "prediction = tf.add(tf.matmul(X, weights['weight']), weights['bias'])\n",
        "\n",
        "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits = prediction, labels = y))\n",
        "\n",
        "optimizer = tf.train.AdamOptimizer(learning_rate).minimize(cost)\n",
        "\n",
        "sess = tf.Session()\n",
        "sess.run(tf.global_variables_initializer())\n",
        "\n"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From <ipython-input-37-23d8191c0381>:3: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "\n",
            "Future major versions of TensorFlow will allow gradients to flow\n",
            "into the labels input on backprop by default.\n",
            "\n",
            "See @{tf.nn.softmax_cross_entropy_with_logits_v2}.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "BsnfU3XhH0n9",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1686
        },
        "outputId": "50807f08-3955-4ef5-fe4e-936e3beba2f8"
      },
      "cell_type": "code",
      "source": [
        "for i in range(100):\n",
        "  epoch_x = training_x\n",
        "  epoch_x = np.reshape(epoch_x, (-1, 4096))\n",
        "  epoch_y = training_y\n",
        "  o, c = sess.run([optimizer, cost], feed_dict = {X : epoch_x, y : epoch_y, learning_rate : .00001})\n",
        "  epoch_loss = c\n",
        "\n",
        "  print('epoch' , i, 'completed out of ', hm_epochs, 'loss ', epoch_loss)"
      ],
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "epoch 0 completed out of  10 loss  1.5383178\n",
            "epoch 1 completed out of  10 loss  1.5383064\n",
            "epoch 2 completed out of  10 loss  1.5382949\n",
            "epoch 3 completed out of  10 loss  1.5382833\n",
            "epoch 4 completed out of  10 loss  1.538272\n",
            "epoch 5 completed out of  10 loss  1.5382607\n",
            "epoch 6 completed out of  10 loss  1.5382491\n",
            "epoch 7 completed out of  10 loss  1.5382376\n",
            "epoch 8 completed out of  10 loss  1.5382261\n",
            "epoch 9 completed out of  10 loss  1.5382147\n",
            "epoch 10 completed out of  10 loss  1.5382032\n",
            "epoch 11 completed out of  10 loss  1.5381914\n",
            "epoch 12 completed out of  10 loss  1.5381798\n",
            "epoch 13 completed out of  10 loss  1.5381682\n",
            "epoch 14 completed out of  10 loss  1.5381566\n",
            "epoch 15 completed out of  10 loss  1.538145\n",
            "epoch 16 completed out of  10 loss  1.5381334\n",
            "epoch 17 completed out of  10 loss  1.5381218\n",
            "epoch 18 completed out of  10 loss  1.5381101\n",
            "epoch 19 completed out of  10 loss  1.5380982\n",
            "epoch 20 completed out of  10 loss  1.5380867\n",
            "epoch 21 completed out of  10 loss  1.538075\n",
            "epoch 22 completed out of  10 loss  1.5380629\n",
            "epoch 23 completed out of  10 loss  1.5380511\n",
            "epoch 24 completed out of  10 loss  1.5380394\n",
            "epoch 25 completed out of  10 loss  1.5380278\n",
            "epoch 26 completed out of  10 loss  1.5380157\n",
            "epoch 27 completed out of  10 loss  1.538004\n",
            "epoch 28 completed out of  10 loss  1.5379921\n",
            "epoch 29 completed out of  10 loss  1.5379801\n",
            "epoch 30 completed out of  10 loss  1.5379682\n",
            "epoch 31 completed out of  10 loss  1.5379562\n",
            "epoch 32 completed out of  10 loss  1.5379443\n",
            "epoch 33 completed out of  10 loss  1.5379324\n",
            "epoch 34 completed out of  10 loss  1.5379204\n",
            "epoch 35 completed out of  10 loss  1.5379084\n",
            "epoch 36 completed out of  10 loss  1.5378965\n",
            "epoch 37 completed out of  10 loss  1.5378845\n",
            "epoch 38 completed out of  10 loss  1.5378723\n",
            "epoch 39 completed out of  10 loss  1.53786\n",
            "epoch 40 completed out of  10 loss  1.5378481\n",
            "epoch 41 completed out of  10 loss  1.5378362\n",
            "epoch 42 completed out of  10 loss  1.5378237\n",
            "epoch 43 completed out of  10 loss  1.5378118\n",
            "epoch 44 completed out of  10 loss  1.5377997\n",
            "epoch 45 completed out of  10 loss  1.5377874\n",
            "epoch 46 completed out of  10 loss  1.5377754\n",
            "epoch 47 completed out of  10 loss  1.5377632\n",
            "epoch 48 completed out of  10 loss  1.537751\n",
            "epoch 49 completed out of  10 loss  1.5377387\n",
            "epoch 50 completed out of  10 loss  1.5377264\n",
            "epoch 51 completed out of  10 loss  1.5377141\n",
            "epoch 52 completed out of  10 loss  1.5377018\n",
            "epoch 53 completed out of  10 loss  1.5376896\n",
            "epoch 54 completed out of  10 loss  1.537677\n",
            "epoch 55 completed out of  10 loss  1.5376647\n",
            "epoch 56 completed out of  10 loss  1.5376524\n",
            "epoch 57 completed out of  10 loss  1.5376401\n",
            "epoch 58 completed out of  10 loss  1.5376277\n",
            "epoch 59 completed out of  10 loss  1.5376152\n",
            "epoch 60 completed out of  10 loss  1.537603\n",
            "epoch 61 completed out of  10 loss  1.5375904\n",
            "epoch 62 completed out of  10 loss  1.5375781\n",
            "epoch 63 completed out of  10 loss  1.5375655\n",
            "epoch 64 completed out of  10 loss  1.5375528\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "epoch 65 completed out of  10 loss  1.5375404\n",
            "epoch 66 completed out of  10 loss  1.5375278\n",
            "epoch 67 completed out of  10 loss  1.5375153\n",
            "epoch 68 completed out of  10 loss  1.5375028\n",
            "epoch 69 completed out of  10 loss  1.5374901\n",
            "epoch 70 completed out of  10 loss  1.5374776\n",
            "epoch 71 completed out of  10 loss  1.537465\n",
            "epoch 72 completed out of  10 loss  1.5374523\n",
            "epoch 73 completed out of  10 loss  1.5374397\n",
            "epoch 74 completed out of  10 loss  1.5374272\n",
            "epoch 75 completed out of  10 loss  1.5374144\n",
            "epoch 76 completed out of  10 loss  1.5374016\n",
            "epoch 77 completed out of  10 loss  1.537389\n",
            "epoch 78 completed out of  10 loss  1.5373763\n",
            "epoch 79 completed out of  10 loss  1.5373634\n",
            "epoch 80 completed out of  10 loss  1.5373509\n",
            "epoch 81 completed out of  10 loss  1.5373378\n",
            "epoch 82 completed out of  10 loss  1.537325\n",
            "epoch 83 completed out of  10 loss  1.5373123\n",
            "epoch 84 completed out of  10 loss  1.5372994\n",
            "epoch 85 completed out of  10 loss  1.5372865\n",
            "epoch 86 completed out of  10 loss  1.5372735\n",
            "epoch 87 completed out of  10 loss  1.5372607\n",
            "epoch 88 completed out of  10 loss  1.5372477\n",
            "epoch 89 completed out of  10 loss  1.5372349\n",
            "epoch 90 completed out of  10 loss  1.5372221\n",
            "epoch 91 completed out of  10 loss  1.537209\n",
            "epoch 92 completed out of  10 loss  1.5371963\n",
            "epoch 93 completed out of  10 loss  1.537183\n",
            "epoch 94 completed out of  10 loss  1.53717\n",
            "epoch 95 completed out of  10 loss  1.5371572\n",
            "epoch 96 completed out of  10 loss  1.5371438\n",
            "epoch 97 completed out of  10 loss  1.537131\n",
            "epoch 98 completed out of  10 loss  1.537118\n",
            "epoch 99 completed out of  10 loss  1.5371048\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "KHedB_aQIs_M",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        },
        "outputId": "a7e03340-ebd4-48d0-ddfa-fd1057cf7033"
      },
      "cell_type": "code",
      "source": [
        "correct = tf.equal(tf.argmax(prediction, 1), tf.argmax(y, 1))\n",
        "\n",
        "accuracy = tf.reduce_mean(tf.cast(correct, 'float'))\n",
        "\n",
        "print('Accuracy:',sess.run(accuracy, feed_dict = {X:np.reshape(testing_x, (-1, 4096)), y:testing_y}))\n"
      ],
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy: 0.9703833\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "6Ho-Ur2EuhPW",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}